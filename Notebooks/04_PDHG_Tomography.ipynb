{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Primal Dual Hybrid Gradient Algorithm: Tomography </center></h1>\n",
    "\n",
    "In this demo, we learn how to use the Primal Dual hubrid algorithm introduced in [ChampollePock](https://hal.archives-ouvertes.fr/hal-00490826/document) for Tomography Reconstruction under an edge-preserving prior, i.e., the __Total variation regularisation__, see [ROF](#ROF). \n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Non-smooth minimisation problem using PDHG algorithm.\n",
    "1. Setup and run PDHG with $L^{1}$ norm regularisation. __(No BlockFramework)__\n",
    "1. Use BlockFunction and setup PDHG within the __Block Framework__.\n",
    "1. Run Total variation reconstruction with different regularising parameters and compared with FBP and SIRT reconstructions.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AcquisitionData, AcquisitionGeometry, AstraProjectorSimple.\n",
    "- BlockOperator, Gradient.\n",
    "- FBP, SIRT, CGLS, Tikhonov.\n",
    "\n",
    "In the figure below, we present 4 different tomography reconstructions that we discuss in this demo. We have seen in the previous demo that the __Tikhonov regularisation__ with $L = \\nabla$ was able to remove the noise but could not preserve the edges. However, this can be achieved with the the total variation reconstruction.\n",
    "\n",
    "<img src=\"images/recon_all_tomo.jpeg\"  width=\"800\"/>\n",
    "\n",
    "The minimisation problem for Total variation Tomography reconstuction is \n",
    "\n",
    "<a id='TomoTV'></a>\n",
    "$$ \\underset{u}{\\operatorname{argmin}} \\frac{1}{2} \\| \\mathcal{A} u - g\\|^{2} + \\alpha\\,\\mathrm{TV}(u) +  \\mathbb{I}_{\\{u>0\\}}(u) $$\n",
    "\n",
    "where,\n",
    "1. The total variation is $$\\mathrm{TV}(u) = \\|\\nabla u \\|_{2,1} = \\sum \\sqrt{ (\\partial_{y}u)^{2} + (\\partial_{x}u)^{2} }$$\n",
    "1. g is the Acqusisition data obtained from the detector.\n",
    "1. $\\mathcal{A}$ is the projection operator ( _Radon transform_ ) that maps from an image-space to an acquisition space, i.e., $\\mathcal{A} : X \\rightarrow Y, $ where X is an __ImageGeometry__ and Y is an __AcquisitionGeometry__.\n",
    "1. $\\alpha$: regularising parameter that measures a trade-off between the fidelity and the regulariser terms.\n",
    "1. $\\mathbb{I}_{\\{u>0\\}}(u) : = \n",
    "\\begin{cases}\n",
    "0, & \\mbox{ if } u>0\\\\\n",
    "\\infty , & \\mbox{ otherwise}\n",
    "\\quad\n",
    "\\end{cases}\n",
    "$, $\\quad$ a positivity constraint for the minimiser $u$.\n",
    "\n",
    "In order to use the PDHG algorithm for the problem above, we need to express our minimisation problem into the following form:\n",
    "\n",
    "<a id='PDHG_form'></a>\n",
    "$$\\min_{u} \\mathcal{F}(K u) + \\mathcal{G}(u)$$\n",
    "\n",
    "where we assume that:\n",
    "\n",
    "1. $\\mathcal{F}$, $\\mathcal{G}$ are __convex__ functionals\n",
    "    \n",
    "    - $\\mathcal{F}: Y \\rightarrow \\mathbb{R}$ \n",
    "    \n",
    "    - $\\mathcal{G}: X \\rightarrow \\mathbb{R}$\n",
    "    \n",
    "    \n",
    "2. $K$ is a continuous linear operator acting from a space X to another space Y :\n",
    "\n",
    "$$K : X \\rightarrow Y \\quad $$ \n",
    "\n",
    "with operator norm  defined as $$\\| K \\| = \\max\\{ \\|K x\\|_{Y} : \\|x\\|_{X}\\leq 1 \\}.$$  \n",
    "\n",
    "**Note**: The Gradient operator  has  $\\|\\nabla\\| = \\sqrt{8} $ and for the projection operator we use the [Power Method](https://en.wikipedia.org/wiki/Power_iteration) to approximate the greatest eigenvalue of $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from ccpi.framework import ImageData, TestData, ImageGeometry, AcquisitionGeometry, AcquisitionData, BlockDataContainer\n",
    "\n",
    "from ccpi.optimisation.functions import L2NormSquared, ZeroFunction, L1Norm, BlockFunction, MixedL21Norm, IndicatorBox, FunctionOperatorComposition\n",
    "from ccpi.optimisation.operators import Gradient, BlockOperator\n",
    "from ccpi.optimisation.algorithms import PDHG, SIRT, CGLS\n",
    "\n",
    "from ccpi.astra.operators import AstraProjectorSimple, AstraProjector3DSimple\n",
    "from ccpi.astra.processors import FBP\n",
    "\n",
    "from skimage.measure import compare_psnr\n",
    "\n",
    "import tomophantom\n",
    "from tomophantom import TomoP2D\n",
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utilities import islicer, link_islicer\n",
    "from utilities.show_utilities import show\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PDHG?\n",
    "\n",
    "In the previous demo, we presented the __Tikhonov regularisation__ for tomography reconstruction, i.e.,\n",
    "\n",
    "<a id='Tikhonov'></a>\n",
    "$$ \\underset{u}{\\operatorname{argmin}} \\|\\mathcal{A} u - g\\|^{2}_{2} + \\alpha^{2}\\|L u\\|^{2}_{2} $$\n",
    "\n",
    "where we set $L = \\nabla $ or $L = \\mathbb{I}$. Due to the $\\|\\cdot\\|^{2}_{2}$ terms, one can observe that the above objective function is differentiable. However, this is not always the case. Consider for example an $L^{1}$ norm for the fidelity, i.e., $\\|\\mathcal{A} u - g\\|_{1}$ or an $L^{1}$ norm of the regulariser i.e., $\\|u\\|_{1}$.\n",
    "\n",
    "Using the __Primal-Dual hybrid gradient algorithm__, we can solve minimisation problems where the objective is not differentiable, and only a convexity is required. \n",
    "\n",
    "The algorithm is described in the [Appendix](#Appendix) and for every iteration, we solve two subproblems, i.e., __primal & dual problems__ where the __[proximal operators](#Proximal)__ have a closed form solution or can be solved efficiently using an iterative solver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In this example, we let $L=\\mathbb{I}$ in [Tikhonov regularisation](#Tikhonov) and replace the $$\\alpha^{2}\\|L u\\|^{2}_{2}\\mbox{  with  } \\alpha\\|u\\|_{1}, $$ \n",
    "\n",
    "which results to a non-differentiable objective function. Hence, we have \n",
    "\n",
    "<a id='Lasso'></a>\n",
    "$$ \\underset{u}{\\operatorname{argmin}} \\|\\mathcal{A} u - g\\|^{2}_{2} + \\alpha\\|u\\|_{1}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to setup and run PDHG\n",
    "\n",
    "In order to setup and run PDHG, we need to define the following:\n",
    "\n",
    "- The operator K.\n",
    "- The function $\\mathcal{F}$ and $\\mathcal{G}$.\n",
    "- Step-sizes $\\sigma$ and $\\tau$ such that $\\sigma\\tau\\|K\\|^{2}<1$, see [Appendix](#Appendix).\n",
    "\n",
    "The setup and run of PDHG:\n",
    "\n",
    "` pdhg = PDHG(f = F, g = G, operator = K, tau = tau, sigma = sigma, max_iterations = maxiter)`\n",
    "\n",
    "` pdhg.run()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to setup and run PDHG algorithm for the above minimisation problem. \n",
    "\n",
    "First, we load a phantom, from the [Tomophantom](https://github.com/dkazanc/TomoPhantom) package. We can\n",
    "choose different 2D,3D & 4D phantoms from this [library](https://github.com/dkazanc/TomoPhantom/tree/master/PhantomLibrary/models). For the well-known Shepp-Logan phantom, use _model = 1_ in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 12 # select a model number from the library\n",
    "N = 200 # set dimension of the phantom\n",
    "path = os.path.dirname(tomophantom.__file__)\n",
    "path_library2D = os.path.join(path, \"Phantom2DLibrary.dat\")\n",
    "\n",
    "phantom = TomoP2D.Model(model, N, path_library2D) \n",
    "\n",
    "# Define image geometry.\n",
    "ig = ImageGeometry(voxel_num_x = N, voxel_num_y = N)\n",
    "im_data = ig.allocate()\n",
    "im_data.fill(phantom)\n",
    "\n",
    "show(im_data, title = 'TomoPhantom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an __AcquisitionGeometry__ and __AcquisitionData__ using a __Projection Operator__ $\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create AcquisitionGeometry and AcquisitionData \n",
    "detectors = N\n",
    "angles = np.linspace(0, np.pi, 180, dtype='float32')\n",
    "ag = AcquisitionGeometry('parallel','2D', angles, detectors)\n",
    "\n",
    "# Create projection operator using Astra-Toolbox. Available CPU/CPU\n",
    "A = AstraProjectorSimple(ig, ag, device = 'gpu')\n",
    "\n",
    "# Create an acqusition data (numerically)\n",
    "sino_num = A.direct(im_data)\n",
    "\n",
    "# Show numerical sinogram\n",
    "show(sino_num, title = 'Numerical Sinogram', labels = ['Detectors','Angle'])\n",
    "\n",
    "# Simulate Gaussian noise for the sinogram\n",
    "gaussian_var = 2\n",
    "gaussian_mean = 0 # or np.mean(sino_num.as_array())\n",
    "n1 = np.random.normal(gaussian_mean, gaussian_var, size = ag.shape)\n",
    "                      \n",
    "sino_noisy = ag.allocate()\n",
    "sino_noisy.fill(n1+sino_num.as_array())\n",
    "show(sino_noisy, title = 'Noisy Sinogram', labels = ['Detectors','Angle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in order to write our [problem](#Lasso) into this [form](#PDHG_form), we let\n",
    "\n",
    "1. $K = \\mathcal{A} \\quad \\Longleftrightarrow \\quad $ `K = A`  \n",
    "\n",
    "1. $\\mathcal{F}: Y \\rightarrow \\mathbb{R}, \\mbox{ with } \\mathcal{F}(z) := \\frac{1}{2}\\| z - g \\|^{2}, \\quad \\Longleftrightarrow \\quad$ ` F = 0.5 * L2NormSquared(sino_noisy)`\n",
    "\n",
    "1. $\\mathcal{G}: X \\rightarrow \\mathbb{R}, \\mbox{ with } \\mathcal{G}(z) := \\|z\\|_{1}, \\quad \\Longleftrightarrow \\quad$ ` G = L1Norm()`.\n",
    "\n",
    "**Note**: In order to use the PDHG algorithm, we do not need to setup the objective function, but rather the functions $\\mathcal{F}, \\mathcal{G}$ and operator $K$ that form the objective.\n",
    "\n",
    "Hence, we can verify that with the above setting we end up with this [form](#PDHG_form) for $x=u$,  $$\\underset{u}{\\operatorname{argmin}} \\frac{1}{2}\\|\\mathcal{A} u - g\\|^{2}_{2} + \\alpha\\|u\\|_{1} = \n",
    "\\underset{u}{\\operatorname{argmin}} \\mathcal{F}(\\mathcal{A}u) + \\mathcal{G}(u) = \\underset{x}{\\operatorname{argmin}} \\mathcal{F}(Kx) + \\mathcal{G}(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define operator K, functions F and G\n",
    "K = A\n",
    "F = 0.5 * L2NormSquared(sino_noisy)\n",
    "\n",
    "alpha = 1e2\n",
    "G = alpha * L1Norm()\n",
    "\n",
    "# Compute operator norm and step-sizes sigma/tau, such that sigma*tau*||K||^{2}<1\n",
    "normK = K.norm()\n",
    "sigma = 1\n",
    "tau = 1/(sigma*normK**2)\n",
    "\n",
    "# Setup and run PDHG\n",
    "pdhg = PDHG(f = F, g = G, operator = K, sigma = sigma, tau = tau, \n",
    "            max_iteration = 500,\n",
    "            update_objective_interval = 100)\n",
    "pdhg.run()\n",
    "\n",
    "recon = pdhg.get_output()\n",
    "\n",
    "# Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "PSNR = compare_psnr(im_data.as_array(), recon.as_array())\n",
    "\n",
    "show(recon, title = 'Tomography reconstruction: L1 regularisation: PSNR = {0:.2f}'.format(PSNR))\n",
    "show(im_data, title = 'Ground Truth')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: PDHG Convergence \n",
    "\n",
    "We say that the PDHG converges if the __Primal-Dual Gap = Primal objective + Dual objective__ $\\rightarrow 0 $, see [here](#Appendix) . \n",
    "\n",
    "Moreover, the speed of convergence of the PDHG algorithm depends heavily on the choise of step-sizes $\\sigma$ and $\\tau$. Notice, that in the above example, we fix $\\sigma=1$ , $\\tau = \\frac{1}{\\sigma\\|K\\|^{2}}$ and terminate the algorithm after 500 iterations. We can use the code above and consider the following cases:\n",
    "\n",
    "1. Run PDHG with $\\sigma=\\frac{1}{\\|K\\|}$, $\\tau = \\frac{1}{\\|K\\|}$ and 500 iterations.\n",
    "1. Run PDHG with $\\sigma=0.001$, $\\tau = \\frac{1}{\\sigma \\|K\\|^2}$ and 500 iterations. What do you observe?\n",
    "1. Change regularising parameter such that $\\alpha\\rightarrow 0$ and $\\alpha\\rightarrow\\infty$. What do you observe for these asympotic choises of $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Variation Regularisation\n",
    "\n",
    "In this section, we would like to use the PDHG algorithm with the Total variation regulariser defined [here](#TomoTV). Compared to the [Tikhonov](#Tikhonov) problem, we use $L = \\nabla$ but with a non-smooth term which is the __MixedL21Norm__ define [here](#TomoTV). In addition, we enforce a positivity constraint for our solution using $\\mathbb{I}_{\\{u>0\\}}(u)$.\n",
    "\n",
    "## BlockFunction\n",
    "\n",
    "In order to setup the above [problem](#TomoTV) we need to use the __BlockFramework__ and in particular the concepts of __BlockOperator__, __BlockDataContainer__ and __BlockFunction__. In the previous demos, we have already discussed BlockOperator and BlockDataContainer. \n",
    "\n",
    "BlockFunction behaves similarly to a BlockOperator/BlockDataContainer but instead of _stacking_ operators or DataContainers we _append_ functions as below:\n",
    "\n",
    "$$ F = [f_{1}, f_{2}]$$\n",
    "\n",
    "For example, let \n",
    "\n",
    "$$\\begin{align}\n",
    "f_{1}: Y \\rightarrow \\mathbb{R}, \\quad f_{1}(z_{1}) = \\alpha\\,\\|z_{1}\\|_{2,1}, \\mbox{ ( the TV term ) }\\\\\n",
    "f_{2}: X \\rightarrow \\mathbb{R}, \\quad f_{2}(z_{2}) = \\frac{1}{2}\\|z_{2} - g\\|_{2}^{2}, \\mbox{ ( the data-fitting term ) }\n",
    "\\end{align}$$\n",
    "\n",
    "and consider $z = (z_{1}, z_{2})\\in Y\\times X$, then \n",
    "<a id='BlockFunction'></a>\n",
    "$$F(z) : = F((z_{1},z_{2}) = f_{1}(z_{1}) + f_{2}(z_{2})$$\n",
    "\n",
    "\n",
    "## Why BlockFunction?\n",
    "\n",
    "With the above form, $F(z)$ is a __separable sum__ of decoupled functions. __So, why do we need to write in this form__? It turns out, that the __proximal operator of a BlockFunction__ can be separated, i.e., \n",
    "\n",
    "$$\\mathrm{prox}_{\\tau F}(z) = \n",
    "\\begin{bmatrix}\n",
    "\\mathrm{prox}_{\\tau f_1}(z_1)\\\\\n",
    "\\mathrm{prox}_{\\tau f_2}(z_2)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\underset{w_{1}\\in Y}{\\operatorname{argmin}} \\tau f_{1}(w_{1}) + \\frac{1}{2}\\|w_{1} - z_{1}\\|^{2}\\\\\n",
    "\\underset{w_{2}\\in X}{\\operatorname{argmin}} \\tau f_{2}(w_{2}) + \\frac{1}{2}\\|w_{2} - z_{2}\\|^{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence, in order to solve the __dual problem__ of the [PDHG](#Appendix) algorithm, we need to solve the two decoupled problems as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDHG for TV regularisation\n",
    "\n",
    "At this stage, we have all the necessary ingredients in order to setup our PDHG algorithm for Total variation tomography reconstruction. We define K as a BlockOperator, with the Gradient and Projection operator:\n",
    "\n",
    "$$ K = \n",
    "\\begin{bmatrix}\n",
    "\\nabla\\\\\n",
    "A\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "` K = BlockOperator(Grad, A)`\n",
    "\n",
    "The function $\\mathcal{F}$, is a [BlockFunction](#BlockFunction) with \n",
    "\n",
    "1. a function $\\alpha\\|\\cdot\\|_{2,1}\\quad\\Longleftrightarrow\\quad$ ` MixedL21Norm() ` term that represents the Total variation regularisation ,\n",
    "\n",
    "1. a function $\\|\\cdot -g \\|_{2}^{2}\\quad\\Longleftrightarrow\\quad$ `L2NormSquared(sino_noisy)` term that represents the data fitting.\n",
    "\n",
    "Hence, $F = [f1, f2] \\quad\\Longleftrightarrow\\quad $ ` F = BlockFunction( MixedL21Norm(), L2NormSquared(sino_noisy))`\n",
    "\n",
    "Finally, we have the function $\\mathcal{G} = \\mathbb{I}_{\\{u>0\\}}(u) \\quad\\Longleftrightarrow\\quad$ ` G = IndicatorBox(lower=0)`\n",
    "\n",
    "Again, we can verify that with the above setting we can express our problem into this [form](#PDHG_form), for $x=u$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{u}{\\operatorname{argmin}}\\alpha\\|\\nabla u\\|_{2,1} + \\frac{1}{2}\\|\\mathcal{A} u - g\\|^{2}_{2} + \\mathbb{I}_{\\{u>0\\}}(u) =  \\underset{u}{\\operatorname{argmin}} f_{1}(\\nabla u) + f_{2}(\\mathcal{A}u) + \\mathbb{I}_{\\{u>0\\}}(u) \\\\ = \\underset{u}{\\operatorname{argmin}} F(\n",
    "\\begin{bmatrix}\n",
    "\\nabla \\\\\n",
    "\\mathcal{A}\n",
    "\\end{bmatrix}u) + \\mathbb{I}_{\\{u>0\\}}(u) = \n",
    "\\underset{u}{\\operatorname{argmin}} \\mathcal{F}(Ku) + \\mathcal{G}(u) = \\underset{x}{\\operatorname{argmin}} \\mathcal{F}(Kx) + \\mathcal{G}(x) \n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Setup and run PDHG algorithm with the Total variation regulariser\n",
    "\n",
    "Follow the steps below:\n",
    "\n",
    "1. Define Gradient operator and BlockOperator K. The ImageGeometry, AcquisitionGeometry and Projection Operator and are already.\n",
    "1. Define BlockFunction F ( do not forget the regularisation parameter )\n",
    "1. Define Function G\n",
    "1. Computer operator norm of K, sigma and tau\n",
    "1. Setup and run PDHG\n",
    "1. Show reconstruction result\n",
    "\n",
    "The reconstruction will look like the figure below after running PDHG for 300 iterations, with $\\sigma=1$, $\\tau = \\frac{1}{\\sigma\\|K\\|^{2}}$ and $\\alpha=50$.\n",
    "\n",
    "<img src=\"images/tv_recon_300iter_alpha_50.png\"  width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradient operator and BlockOperator K\n",
    "\n",
    "### START CODE HERE (2 lines) ### \n",
    "    \n",
    "    # Grad = Gradient()\n",
    "    # K = BlockOperator(  ,  )\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Define BlockFunction F using the MixedL21Norm() and the L2NormSquared() \n",
    "\n",
    "### START CODE HERE (5 lines) ### \n",
    "    \n",
    "    # alpha = 50\n",
    "    # f1 = MixedL21Norm()\n",
    "    # f2 = L2NormSquared()\n",
    "    # F = BlockFunction(  ,  )\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Define BlockFunction G, as a positivity constraint\n",
    "\n",
    "### START CODE HERE (1 lines) ### \n",
    "    \n",
    "    # G = IndicatorBox()\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1 (3 lines)  \n",
    "\n",
    "### START CODE HERE (1 lines) ### \n",
    "\n",
    "    # normK = \n",
    "    # sigma = \n",
    "    # tau = \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Setup and run PDHG\n",
    "\n",
    "### START CODE HERE (2 lines) ### \n",
    "\n",
    "    # pdhg = ..\n",
    "    # pdhg... \n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradient Operator and BlockOperator \n",
    "Grad = Gradient(...)\n",
    "K = BlockOperator(...,...)\n",
    "\n",
    "# Define BlockFunction F using the MixedL21Norm() and the L2NormSquared()\n",
    "alpha = ...\n",
    "f1 = alpha * MixedL21Norm()\n",
    "f2 = 0.5 * L2NormSquared(...)\n",
    "F = BlockFunction(..,...)\n",
    "\n",
    "# Define BlockFunction G, as a positivity constraint\n",
    "G = IndicatorBox(...)\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1\n",
    "normK = K.norm()\n",
    "\n",
    "sigma = 1\n",
    "tau = 1/(sigma*normK**2)\n",
    "\n",
    "pdhg = PDHG(f = ..., g = ..., operator = ..., tau = ..., sigma = ..., \n",
    "            max_iteration = 300, update_objective_interval = 100)\n",
    "pdhg.run(verbose = True)\n",
    "\n",
    "pdhg_tvrecon = pdhg.get_output()\n",
    "\n",
    "# Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "PSNR = compare_psnr(im_data.as_array(), pdhg_tvrecon.as_array(), data_range=np.max(im_data.as_array()))\n",
    "\n",
    "show(pdhg_tvrecon, title = 'TV reconstruction: PSNR = {0:.2f}'.format(PSNR))\n",
    "show(im_data, title = 'Ground Truth')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(1,N,N), pdhg_tvrecon.as_array()[int(N/2),:])\n",
    "plt.title('Middle Line Profile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Find the best regulariser for TV reconstruction and compare with FBP and SIRT algorithms\n",
    "\n",
    "For this exercise, we ask the user to\n",
    "\n",
    "1. Run TV reconstruction with 5 different regularising parameters, e.g., $\\alpha = [1, 10, 50, 500, 1e3, 1e4]$.\n",
    "1. Choose the best reconstruction according to the highest PSNR.\n",
    "1. Replace the _MixedL21Norm_ with the _L2NormSquared_ in the _BlockFunction_ definition. Can you identify this minimisation problem?What do you observe\n",
    "1. Run FBP and SIRT.\n",
    "\n",
    "__Hint__: For the prima/dual step size we set:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma_{i} = \\frac{C\\alpha_{i}}{\\|K\\|}\\\\\n",
    "\\tau_{i} = \\frac{1}{C\\alpha_{i}\\|K\\|}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we would like to setup and run FBP and SIRT reconstructions. Recall, for FBP we write\n",
    "\n",
    "` fbp = FBP(ig, ag, filter_type = 'ram-lak') `\n",
    "\n",
    "` fbp.set_input(sino_noisy)`\n",
    "\n",
    "` fbp_recon = fbp.get_output()` \n",
    "\n",
    "and for SIRT\n",
    "\n",
    "` x_init = ig.allocate()` \n",
    "\n",
    "` sirt = SIRT(x_init = x_init, operator = A, data=sino_noisy, constraint = IndicatorBox(lower=0),max_iteration = 50)`\n",
    "\n",
    "` sirt.run(verbose=False) `\n",
    "\n",
    "` sirt_recon = sirt.get_output() `\n",
    "\n",
    " __(Reminder)__ : If $\\mathcal{A} u = g$, then:  $$u^{k+1} = \\mathcal{P}_{C}(u^{k} + D \\mathcal{A}^{T} M ( g - \\mathcal{A} u^{k}))$$ where,\n",
    " $$\n",
    "    \\begin{cases}\n",
    "    M = \\frac{\\mathbb{1}}{\\mathcal{A}\\mathbb{1}}, \\quad m_{ii} = \\frac{1}{\\sum_{j} a_{ij}}, \\quad\\mbox{ sum over columns }\\\\\n",
    "    D = \\frac{\\mathbb{1}}{\\mathcal{A}^{T}\\mathbb{1}}, \\quad d_{jj} = \\frac{1}{\\sum_{i} a_{ij}}\\quad\\mbox{ sum over rows }\\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "and $\\mathcal{P}_{C}$ is a projection onto a convex set C. For this demo, we will use a positivity constraint.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of possible regularisation parameters\n",
    "\n",
    "### START CODE HERE (1 line) ### \n",
    "    \n",
    "    # alpha = [ , , , , , ]\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1\n",
    "\n",
    "### START CODE HERE (1 line) ### \n",
    "    \n",
    "    # normK = \n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Allocate space for all the TV reconstructions and PSNRs\n",
    "\n",
    "### START CODE HERE (2 lines) ### \n",
    "    \n",
    "    # recon_all = np.zeros( ((len(alpha),) + ig.shape))\n",
    "    # PSNR = np.zeros(len(alpha)) \n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Define function G, and data-fitting term that is an function-element of the BlockFunction\n",
    "\n",
    "### START CODE HERE (2 lines) ### \n",
    "    \n",
    "    # G = IndicatorBox()\n",
    "    # f2 = L2NormSquared()\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Define function G, and data-fitting term that is an function-element of the BlockFunction\n",
    "\n",
    "### START CODE HERE (3 lines) ### \n",
    "\n",
    "    # C = 10\n",
    "    \n",
    "    #for i in range(len(alpha)):\n",
    "\n",
    "    #    sigma = C*alpha[i]/normK\n",
    "    #    tau = 1/(C*alpha[i]*normK)    \n",
    "\n",
    "    #    f1 = ...\n",
    "    #    F = BlockFunction(..., ...)\n",
    "\n",
    "    #    pdhg = PDHG(f = ..., g = ..., operator = ..., tau = ..., sigma = ..., \n",
    "    #            max_iteration = 1000, update_objective_interval = 500)\n",
    "    #    pdhg.run(verbose = True)\n",
    "\n",
    "    #    recon_all[i] = pdhg.get_output().as_array()\n",
    "\n",
    "        # Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "    #    PSNR[i] =  compare_psnr(im_data.as_array(), recon_all[i] )\n",
    "\n",
    "    #    show(pdhg.get_output(), title = 'TV reconstruction: alpha = {0:.2f}'.format(alpha[i]))\n",
    "    #    plt.pause(0.5)\n",
    "    \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BlockFunction F using the MixedL21Norm() and the L2NormSquared()\n",
    "alpha = [..., ..., ..., ..., ..., ...]\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1\n",
    "normK = ...\n",
    "\n",
    "recon_all = np.zeros( ((len(alpha),) + ig.shape))\n",
    "PSNR = np.zeros(len(alpha))\n",
    "\n",
    "# Define BlockFunction G, as a positivity constraint\n",
    "G = IndicatorBox(...)\n",
    "f2 = 0.5 * L2NormSquared(...)\n",
    "\n",
    "for i in range(len(alpha)):\n",
    "    \n",
    "    sigma = C*alpha[i]/normK\n",
    "    tau = 1/(C*alpha[i]*normK)    \n",
    "    \n",
    "    f1 = ...\n",
    "    F = BlockFunction(f1, f2)\n",
    "\n",
    "    pdhg = PDHG(f = ..., g = ..., operator = ..., tau = ..., sigma = ..., \n",
    "            max_iteration = 1000, update_objective_interval = 500)\n",
    "    pdhg.run(verbose = True)\n",
    "\n",
    "    recon_all[i] = pdhg.get_output().as_array()\n",
    "\n",
    "    # Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "    PSNR[i] =  compare_psnr(im_data.as_array(), recon_all[i] )\n",
    "    \n",
    "    show(pdhg.get_output(), title = 'TV reconstruction: alpha = {0:.2f}'.format(alpha[i]))\n",
    "    plt.pause(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBP reconstruction\n",
    "fbp = FBP(ig, ag, filter_type = 'ram-lak')\n",
    "fbp.set_input(sino_noisy)\n",
    "fbp_recon = fbp.get_output()\n",
    "show(fbp_recon, title = 'FBP reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SIRT algoritm, with positivity constraint\n",
    "\n",
    "# Setup and run the SIRT algorithm  \n",
    "x_init = ig.allocate()  \n",
    "sirt = SIRT(x_init = x_init, operator = A, data=sino_noisy, constraint = IndicatorBox(lower=0),\n",
    "           max_iteration = 50)\n",
    "sirt.run(verbose=False)\n",
    "\n",
    "sirt_recon = sirt.get_output()\n",
    "show(sirt_recon, title = 'SIRT reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The parameters are {}  '.format(alpha))\n",
    "print('The PSNR values {}  '.format(PSNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import islicer\n",
    "stack_recon = ImageData(np.array([fbp_recon.as_array(), \n",
    "                                  sirt_recon.as_array(),\n",
    "                                  recon_all[1],\n",
    "                                  recon_all[2],\n",
    "                                  im_data.as_array()]))\n",
    "\n",
    "islicer(stack_recon, 0, minmax=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>End of Demo </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "# Appendix\n",
    "\n",
    "The PDHG algorithm is described below :\n",
    "    \n",
    "<center> Choose step-sizes $\\tau$, $\\sigma$, such that $\\tau\\sigma\\|K\\|^{2}<1$ </center> \n",
    "<center> Choose $\\theta\\in[0,1]$ </center>\n",
    "<center> Let $(x^{0}, y^{0})\\in X\\times Y$ and $\\overline{x}^{0} = x^{0}$ </center>\n",
    "<br>\n",
    "\\begin{align}\n",
    "    y^{n+1}  & = \\mathrm{prox}_{\\sigma\\mathcal{F}^{*}}( y^{n} + \\sigma K \\overline{x}^{n} )\\quad \\mbox{ (Dual Problem)}\\tag{1}\\\\[10pt]\n",
    "    x^{n+1}  & = \\mathrm{prox}_{\\tau\\mathcal{G}^{*}} ( x^{n} - \\tau K^{*} \\overline{x}^{n} )\\quad \\mbox{ (Primal Problem) }\\tag{2}\\\\[10pt]\n",
    "    \\overline{x}^{n+1} & = x^{n+1} + \\theta ( x^{n+1} - x^{n} )\\quad\\mbox{ (Over-relaxation step) }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Proximal'></a>\n",
    "__**Proximal Operator**__: Let $\\lambda>0$ and $f$ a convex function. Then\n",
    "\n",
    "$$\n",
    "z : = \\mbox{prox}_{\\lambda f}(x) = \\underset{z}{\\operatorname{argmin}} \\frac{1}{2}\\|z - x \\|^{2} + \\lambda f(z) \n",
    "$$\n",
    " \n",
    "__**Note**__: We assume that proximal operators have a closed form solution or can be solved efficiently using an iterative algorithm.\n",
    "\n",
    "__**Primal-Dual Gap (PD)**__:\n",
    "\n",
    "$$\\mbox{PD} = \\mathcal{F}(Kx) + \\mathcal{G}(x) + \\mathcal{F}^{*}(y) + \\mathcal{G}(-K^{T}y) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
